# Solution

First thing we do is examine the binary:
```
$ file ./col
 ./col: setuid ELF 32-bit LSB executable, Intel 80386, version 1 (SYSV), dynamically linked, interpreter /lib/ld-linux.so.2, for GNU/Linux 2.6.24, BuildID[sha1]=05a10e253161f02d8e6553d95018bc82c7b531fe, not stripped
```

As expected it's a 32-bit linux executable. When we look at the source code we can 
see that the program is checking some input against a simple hashing algorithm. The
algorith takes the integer value of each of the five 4-byte blocks of our 20-byte 
input, sums them together, and checks if that is equal to 0x21DD09EC. 


To construct an input that satisfies this, we will need to make sure that none of 
the 20 bytes are zero, otherwise this would terminate the string early. 


Since 0x21DD09EC is not cleanly divisible by 5, at least two of the ints must be
different. To come up with an appropriate sum, add 4 ints together that are all equal
and then add in the remainder as the 5th int. The 4 ints must be at least as big as
0x01010101 to avoid having bytes in them equal to zero, and must be small enough to
not make the 5th int have its most significant byte equal to 0x00. 


An arbitrary value that satisfies this crieria is 0x01111111. Adding 4 of these
gives yields 0x04444444, and a remainder of 0x1d98c5a8. Thus, the resulting string
is: 
```
$ python -c 'print struct.pack("I", 0x01111111)*4 + struct.pack("I", 0x1d98c5a8)'
```

